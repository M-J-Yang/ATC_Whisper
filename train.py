"""
Whisper å¾®è°ƒè®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒåˆ†å¸ƒå¼ + fp16 + ç¼“å­˜ä¼˜åŒ–ï¼‰
é€‚é… RTX 3090 / HuggingFace Transformers >= 4.44
"""

import os
import json
import torch
import numpy as np
import argparse
from datasets import load_from_disk
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
from evaluate import load as load_metric
from dataclasses import dataclass
from typing import Dict, Any, List, Union
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="Whisper å¾®è°ƒè®­ç»ƒ")
parser.add_argument("--unfreeze-encoder-layers", type=int, default=0,
                    help="è§£å†» encoder çš„æœ€å N å±‚è¿›è¡Œè®­ç»ƒ (0=å…¨éƒ¨å†»ç»“)")
args = parser.parse_args()

# ======================================
# Data Collator for Speech Seq2Seq
# ======================================
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """æ•°æ®æ•´ç†å™¨ï¼šåŠ¨æ€ padding éŸ³é¢‘ç‰¹å¾å’Œæ–‡æœ¬æ ‡ç­¾ï¼ˆç‰¹å¾å·²é¢„æå–ï¼‰"""
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # ç‰¹å¾å·²ç»æ˜¯ list æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸º tensor
        # input_features: (batch_size, 80, 3000) - mel spectrogram
        input_features = [torch.tensor(feature["input_features"]) for feature in features]
        label_features = [torch.tensor(feature["labels"]) for feature in features]

        # Pad éŸ³é¢‘ç‰¹å¾åˆ°ç›¸åŒé•¿åº¦
        batch = {}
        batch["input_features"] = torch.stack(input_features)

        # Pad æ ‡ç­¾åˆ°ç›¸åŒé•¿åº¦
        max_label_length = max(len(l) for l in label_features)
        padded_labels = []
        for labels in label_features:
            padding_length = max_label_length - len(labels)
            if padding_length > 0:
                padded_labels.append(torch.cat([
                    labels,
                    torch.full((padding_length,), -100, dtype=labels.dtype)
                ]))
            else:
                padded_labels.append(labels)

        labels = torch.stack(padded_labels)

        # å¦‚æœæ‰€æœ‰åºåˆ—éƒ½ä»¥ bos token å¼€å¤´ï¼Œç§»é™¤å®ƒï¼ˆWhisper ä¸éœ€è¦ï¼‰
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

# ======================================
# âœ… 1. åŠ è½½é…ç½®
# ======================================
CONFIG_PATH = "./config.yaml"
import yaml
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ======================================
# âœ… 2. åŠ è½½æ¨¡å‹ä¸å¤„ç†å™¨
# ======================================
# æ ¹æ®é…ç½®æ„å»ºæ¨¡å‹åç§°
model_type = config["model"]["type"]
if model_type == "whisper":
    whisper_size = config["model"]["whisper_size"]
    model_name = f"openai/whisper-{whisper_size}"
else:
    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

logger.info(f"åŠ è½½æ¨¡å‹ {model_name}...")
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# å†»ç»“/è§£å†» encoder å±‚
if args.unfreeze_encoder_layers > 0:
    logger.info(f"å†»ç»“ encoderï¼Œä»…è§£å†»æœ€å {args.unfreeze_encoder_layers} å±‚")
    # å†»ç»“æ‰€æœ‰ encoder å‚æ•°
    for param in model.model.encoder.parameters():
        param.requires_grad = False
    # è§£å†»æœ€å N å±‚
    total_layers = len(model.model.encoder.layers)
    for i in range(total_layers - args.unfreeze_encoder_layers, total_layers):
        for param in model.model.encoder.layers[i].parameters():
            param.requires_grad = True
    logger.info(f"  - Encoder æ€»å±‚æ•°: {total_layers}")
    logger.info(f"  - è§£å†»å±‚: {total_layers - args.unfreeze_encoder_layers} åˆ° {total_layers - 1}")
elif args.unfreeze_encoder_layers == 0:
    logger.info("å†»ç»“æ•´ä¸ª encoderï¼Œä»…è®­ç»ƒ decoder")
    for param in model.model.encoder.parameters():
        param.requires_grad = False
else:
    logger.info("è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼ˆencoder + decoderï¼‰")

model.to(device)

# ======================================
# âœ… 3. åŠ è½½æ•°æ®é›†ï¼ˆä»ç¼“å­˜ï¼Œç‰¹å¾å·²é¢„æå–ï¼‰
# ======================================
logger.info("ä»ç¼“å­˜åŠ è½½æ•°æ®é›†ï¼ˆç‰¹å¾å·²é¢„æå–ï¼‰...")
# ä½¿ç”¨ output_dir + processed_data è·¯å¾„
dataset_path = os.path.join(config["output"]["output_dir"], "processed_data")
logger.info(f"æ•°æ®é›†è·¯å¾„: {dataset_path}")
dataset = load_from_disk(dataset_path)

# æ•°æ®é›†å·²åŒ…å« input_features å’Œ labelsï¼Œæ— éœ€å†å¤„ç†
logger.info("âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œç‰¹å¾å·²é¢„æå–")
logger.info(f"   - è®­ç»ƒé›†: {len(dataset['train'])} æ¡")
logger.info(f"   - éªŒè¯é›†: {len(dataset['val'])} æ¡")
logger.info(f"   - æµ‹è¯•é›†: {len(dataset['test'])} æ¡")

# ======================================
# âœ… 4. å®šä¹‰è¯„ä¼°æŒ‡æ ‡
# ======================================
wer_metric = load_metric("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    if isinstance(pred_ids, tuple):  # ğŸ”§ ä¿®å¤ tuple é”™è¯¯
        pred_ids = pred_ids[0]

    label_ids = pred.label_ids
    pred_ids = np.where(pred_ids == -100, processor.tokenizer.pad_token_id, pred_ids)
    label_ids = np.where(label_ids == -100, processor.tokenizer.pad_token_id, label_ids)

    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# ======================================
# âœ… 5. è®­ç»ƒå‚æ•°
# ======================================
train_args = config["training"]
output_dir = config["output"]["output_dir"]

# ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®ï¼ˆä» YAML è¯»å–å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
learning_rate = float(train_args["learning_rate"])
weight_decay = float(train_args["weight_decay"])
batch_size = int(train_args["batch_size"])
gradient_accumulation_steps = int(train_args["gradient_accumulation_steps"])
warmup_steps = int(train_args["warmup_steps"])
num_train_epochs = int(train_args["epochs"])

training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    warmup_steps=warmup_steps,
    num_train_epochs=num_train_epochs,
    logging_dir=os.path.join(output_dir, "logs"),
    logging_steps=100,
    save_total_limit=2,
    predict_with_generate=True,  # âœ… å¿…é¡»ä¸º True æ‰èƒ½åœ¨è¯„ä¼°æ—¶ç”Ÿæˆæ–‡æœ¬
    fp16=True,
    gradient_checkpointing=False,
    dataloader_num_workers=int(config["system"]["num_workers"]),
    dataloader_pin_memory=bool(config["system"]["pin_memory"]),
    report_to="none",
    generation_max_length=225,
)

logger.info(f"è®­ç»ƒé…ç½®:")
logger.info(f"  - Batch size: {batch_size}")
logger.info(f"  - Gradient accumulation: {gradient_accumulation_steps}")
logger.info(f"  - Effective batch size: {batch_size * gradient_accumulation_steps}")
logger.info(f"  - Learning rate: {learning_rate}")
logger.info(f"  - Epochs: {num_train_epochs}")


# ======================================
# âœ… 6. æ„å»º Trainer
# ======================================
# åˆ›å»º data collator
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["val"],
    data_collator=data_collator,  # âœ… ä½¿ç”¨æ­£ç¡®çš„ data collator
    tokenizer=processor.tokenizer,  # âœ… ä¼ å…¥ tokenizerï¼ˆç”¨äºä¿å­˜ï¼‰
    compute_metrics=compute_metrics,
)

# ======================================
# âœ… 7. å¯åŠ¨è®­ç»ƒ
# ======================================
logger.info("=" * 60)
logger.info("å¼€å§‹è®­ç»ƒ...")
logger.info("=" * 60)
trainer.train()

# ======================================
# âœ… 8. ä¿å­˜æ¨¡å‹
# ======================================
logger.info("è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹...")
final_model_path = os.path.join(config["output"]["output_dir"], "final_model")
trainer.save_model(final_model_path)
processor.save_pretrained(final_model_path)
logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
